{"cells":[{"cell_type":"markdown","source":["# Language Understanding\n","\n","Increasingly, we expect computers to be able to use AI in order to understand spoken or typed commands in natural language. For example, you might want to implement a home automation system that enables you to control devices in your home by using voice commands such as \"switch on the light\" or \"put the fan on\", and have an AI-powered device understand the command and take appropriate action.\n","\n","![A robot listening](./images/language_understanding.jpg)\n","\n","## Create Authoring and Prediction Resources\n","\n","Microsoft cognitive services includes the Language Understanding service, which enables you to define *intents* that are applied to *entities* based on *utterances*. You can use either a **Language Understanding** or  **Cognitive Services** resource to *publish* a Language Understanding app, but you must create a separate **Language Understanding** resource for *authoring* the app.\n","\n","1. In another browser tab, open the Azure portal at [https://portal.azure.com](https://portal.azure.com), signing in with your Microsoft account.\n","2. Click **+ Create a resource**, and search for *Language Understanding*.\n","3. In the list of services, click **Language Understanding**.\n","4. In the **Language Understanding** blade, click **Create**.\n","5. In the **Create** blade, enter the following details and click **Create**\n","  - **Create option**: Both\n","  - **Name**: *A unique name for your service*\n","  - **Subscription**: *Select your Azure subscription*\n","  - **Resource Group**: *Select an existing resource group or create a new one*\n","  - **Authoring location**: *Select any available location*\n","  - **Authoring pricing tier**: F0\n","  - **Runtime location**: *Same as authoring location*\n","  - **Runtime pricing tier**: F0\n","6. Wait for the resources to be created, and note that two Language Understanding resources are provisioned; one for authoring, and another for prediction. You can view these by navigating to the resource group where you created them.\n","\n","### Create a Language Understanding App\n","\n","To implement natural language understanding with Language Understanding, you create an app; and then add entities, intents, and utterances to define the commands you want the app to understand:\n","\n","1. In a new browser tab, open the Language Understanding portal at [https://www.luis.ai](https://www.luis.ai), and sign in using the Microsoft account associated with your Azure subscription. If this is the first time you have signed into the Language Understanding portal, you may need to grant the app some permissions to access your account details. Then complete the *Welcome* steps by selecting the existing Language Understanding authoring resource you just created in your Azure subscription. \n","2. Open the **My Apps** page, and select your subscription and Language Understanding authoring resource. Then create a new app for conversation with the following settings:\n","  - **Name**: Home Automation\n","  - **Culture**: English\n","  - **Description**: Simple home automation\n","  - **Prediction resource**: *Your Language Understanding prediction resource*\n","3. If a panel with tips for creating an effective Language Understanding app is displayed, close it.\n","\n","### Create an Entity\n","\n","An *entity* is a thing that your language model can identify and do something with. In this case, your Language Understanding app will be used to control various *devices* in the office, such as lights or fans; so you'll create a *device* entity that includes a list of the types of device that you want the app to work with. For each device type, you'll create a sublist that identifies the name of the device (for example *light*) and any synonyms that might be used to refer to this type of device (for example *lamp*).\n","\n","1. In the Language Understanding page for your app, in the pane on the left, click **Entities**. Then click **Create**, and create a new entity named **device**, select the **List** type, and click **Create**.\n","2. In the **List items** page, under **Normalized Values**, type **light**, then press ENTER.\n","3. After the **light** value has been added, under **Synonyms**, type **lamp** and press ENTER.\n","4. Add a second list item named **fan** with the synonym **AC**.\n","\n","### Create Intents\n","\n","An *intent* is an action you want to perform on one or more entities - for example, you might want to switch a light on, or turn a fan off. In this case, you'll define two intents: one to switch a device on, and another to switch a device off. For each intent, you'll specify sample *utterances* that indicate the kind of language used to indicate the intent.\n","\n","1. In the pane on the left, click **Intents**. Then click **Create**, and add an intent with the name **switch_on** and click **Done**.\n","2. Under the **Examples** heading and the **Example user input** subheading, type the utterance ***turn the light on*** and press **Enter** to submit this utterance to the list.\n","3. In the *turn the light on* utterance, click the word \"light\", and assign it to the **device** entity's **light** value.\n","4. Add a second utterance to the **switch_on** intent, with the phrase ***turn the fan on***. Then assign the word \"fan\" to the **device** entity's **fan** value.\n","5. In the pane on the left, click **Intents** and click **Create**, to add a second intent with the name **switch_off**.\n","6. In the **Utterances** page for the **switch_off** intent, add the utterance ***turn the light off*** and assign the word \"light\" to the **device** entity's **light** value.\n","7. Add a second utterance to the **switch_off** intent, with the phrase ***turn the fan off***. Then connect the word \"fan\" to the **device** entity's **fan** value.\n","\n","### Train and Test the Language Model\n","\n","Now you're ready to use the data you've provided in the form of entities, intents, and uterances to train the language model for your app.\n","\n","1. At the top of the Language Understanding page for your app, click **Train** to train the language model\n","2. When the model is trained, click **Test**, and use the Test pane to view the predicted intent for the following phrases:\n","    * *switch the light on*\n","    * *turn off the fan*\n","    * *turn the lamp off*\n","    * *switch on the AC*\n","3. Close the Test pane.\n","    \n","### Publish the Model and Configure Endpoints\n","\n","To use your trained model in a client application, you must publish it as an endpoint to which the client applications can send new utterances; from which intents and entitites will be predicted.\n","\n","1. At the top of the Language Understanding page for your app, click **Publish**. Then select **Production slot** and click **Done**.\n","2. After the model has been published, at the top of the Language Understanding page for your app, click **Manage**. Then on the **Application Information** tab, note the **Application ID** for your app. Copy this and paste it in the code below to replace **YOUR_LU_APP_ID**.\n","3. On the **Azure Resources** tab, note the **Primary key** and **Endpoint URL** for your prediction resource. Copy these and paste them into the code below, replacing **YOUR_LU_KEY** and **YOUR_LU_ENDPOINT**.\n","4. Run the cell below by clicking its **Run cell** (&#9655;) button (to the left of the cell), and when prompted, enter the text *turn the light on*. The text is interpreted by your Language Understanding model and an appropriate image is displayed.\n"],"metadata":{}},{"cell_type":"code","source":["from python_code import luis\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import os\n","%matplotlib inline\n","\n","try:\n","    # Set up API configuration\n","    luis_app_id = 'YOUR_LU_APP_ID'\n","    luis_key = 'YOUR_LU_KEY'\n","    luis_endpoint = 'YOUR_LU_ENDPOINT'\n","\n","    # prompt for a command\n","    command = input('Please enter a command: \\n')\n","\n","    # get the predicted intent and entity (code in python_code.home_auto.py)\n","    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, command)\n","\n","    # display an appropriate image\n","    img_name = action + '.jpg'\n","    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n","    plt.axis('off')\n","    plt. imshow(img)\n","except Exception as ex:\n","    print(ex)"],"outputs":[],"execution_count":null,"metadata":{"tags":[],"gather":{"logged":1599696381331}}},{"cell_type":"markdown","source":["Re-run the cell above, trying the following phrases:\n","\n","* *turn on the light*\n","* *put the lamp off*\n","* *switch the fan on*\n","* *switch the light on*\n","* *switch off the light*\n","* *turn off the fan*\n","* *switch the AC on*\n","\n","> **Note**: If you're curious about the code used to retrieve the intents and entitites from your Language Understanding app, look at the **luis.py** file in the **python_code** folder."],"metadata":{}},{"cell_type":"markdown","source":["## Add Voice Control\n","\n","So far, we've seen how analyze text; but increasingly AI systems enable humans to communicate with software services through speech recognition. To support this, the **Speech** cognitive service provides a simple way to transcribe spoken language into text.\n","\n","### Create a Cognitive Services Resource\n","\n","If you don't already have one, use the following steps to create a **Cognitive Services** resource in your Azure subscription:\n","\n","1. In another browser tab, open the Azure portal at [https://portal.azure.com](https://portal.azure.com), signing in with your Microsoft account.\n","2. Click the **&#65291;Create a resource** button, search for *Cognitive Services*, and create a **Cognitive Services** resource with the following settings:\n","    - **Name**: *Enter a unique name*.\n","    - **Subscription**: *Your Azure subscription*.\n","    - **Location**: *Any available location*.\n","    - **Pricing tier**: S0\n","    - **Resource group**: *Create a resource group with a unique name*.\n","3. Wait for deployment to complete. Then go to your cognitive services resource, and on the **Quick start** page, note the keys and endpoint. You will need these to connect to your cognitive services resource from client applications.\n","\n","### Get the Key and Endpoint for your Cognitive Services Resource\n","\n","To use your cognitive services resource, client applications need its  endpoint and authentication key:\n","\n","1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n","2. Copy the **Endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n","3. Copy the **Location** for your resource and paste it in the code below, replacing **YOUR_COG_REGION**    .\n","4. Run the code in the cell below. "],"metadata":{}},{"cell_type":"code","source":["cog_key = 'YOUR_COG_KEY'\n","cog_endpoint = 'YOUR_COG_ENDPOINT'\n","cog_region = 'YOUR_COG_REGION'\n","\n","print('Ready to use cognitive services in {} using key {}'.format(cog_region, cog_key))"],"outputs":[],"execution_count":null,"metadata":{"tags":[],"gather":{"logged":1599696409914}}},{"cell_type":"markdown","source":["To use the Speech service in your Cognitive Services resource, you'll need to install the Azure Cognitive Services Speech SDK."],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["!pip install azure.cognitiveservices.speech"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["Now run the cell below to transcribe speech from an audio file, and use it as a command for your Language Understanding app."],"metadata":{}},{"cell_type":"code","source":["from python_code import luis\n","import os\n","import IPython\n","import os\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n","\n","try:   \n","\n","    # Get spoken command from audio file\n","    file_name = 'light-on.wav'\n","    audio_file = os.path.join('data', 'luis', file_name)\n","\n","    # Configure speech recognizer\n","    speech_config = SpeechConfig(cog_key, cog_region)\n","    audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n","    speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n","\n","    # Use a one-time, synchronous call to transcribe the speech\n","    speech = speech_recognizer.recognize_once()\n","\n","    # Get the predicted intent and entity (code in python_code.home_auto.py)\n","    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, speech.text)\n","\n","    # Get the appropriate image\n","    img_name = action + '.jpg'\n","\n","    # Play audio and display image\n","    IPython.display.display(IPython.display.Audio(audio_file, autoplay=True),\n","                            IPython.display.Image(data=os.path.join(\"data\", \"luis\" ,img_name)))\n","except Exception as ex:\n","    print(ex)"],"outputs":[],"execution_count":null,"metadata":{"tags":[],"gather":{"logged":1599696420498}}},{"cell_type":"markdown","source":["Try modifying the cell above to use the **light-off.wav** audio file.\n","\n","## Learn More\n","\n","Learn more about Language Understanding in the [service documentation](https://docs.microsoft.com/azure/cognitive-services/luis/)"],"metadata":{}}],"metadata":{"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3-azureml","language":"python","display_name":"Python 3.6 - AzureML"},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}